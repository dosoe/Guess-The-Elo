{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "#df = pd.read_csv(\"../Analyzed_Games/twic1556_15_analyzed.csv\")\n",
    "#df=pd.read_csv(\"../Analyzed_Games/test2_15_analyzed.csv\")\n",
    "#df= pd.read_csv(\"../Analyzed_Games/twic920_15_analyzed.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning data (use Dorian's cleaning function instead of this for better results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of games removed: 11\n"
     ]
    }
   ],
   "source": [
    "initial_game_count = df['GameID'].nunique()\n",
    "\n",
    "# Step 1: Identify GameIDs with valid 'Result'\n",
    "valid_result_games = df[df['Result'].isin(['1-0', '0-1', '1/2-1/2'])]['GameID'].unique()\n",
    "\n",
    "# Step 2: Identify GameIDs with no missing 'WhiteFideId' or 'BlackFideId'\n",
    "fide_valid_games = df.dropna(subset=['WhiteFideId', 'BlackFideId'])['GameID'].unique()\n",
    "\n",
    "# Step 3: Find the intersection of valid games\n",
    "valid_games = np.intersect1d(valid_result_games, fide_valid_games)\n",
    "\n",
    "# Step 4: Filter the DataFrame to include only valid games\n",
    "df_cleaned = df[df['GameID'].isin(valid_games)].copy()\n",
    "\n",
    "# Record the final number of unique games\n",
    "final_game_count = df_cleaned['GameID'].nunique()\n",
    "\n",
    "# Calculate the number of games removed\n",
    "removed_games = initial_game_count - final_game_count\n",
    "\n",
    "# Reset the index\n",
    "df_cleaned = df_cleaned.reset_index(drop=True)\n",
    "\n",
    "# Create a mapping from old GameID to new sequential GameID\n",
    "unique_games = df_cleaned['GameID'].unique()\n",
    "game_id_mapping = {old_id: new_id for new_id, old_id in enumerate(unique_games, start=1)}\n",
    "\n",
    "# Apply the mapping to fix 'GameID'\n",
    "df_cleaned['GameID'] = df_cleaned['GameID'].map(game_id_mapping)\n",
    "\n",
    "# Save the cleaned DataFrame to a new CSV file (optional)\n",
    "# df_cleaned.to_csv(\"../huge_analyzed_games/combined_analyzed_games_cleaned.csv\", index=False)\n",
    "\n",
    "# Print the number of games removed\n",
    "print(f\"Number of games removed: {removed_games}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Winning Chances column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['Evaluation'] = df['Evaluation'].astype(str).str.strip()\n",
    "df['PlayerToMove'] = np.where(df['MoveNumber'] % 2 == 1, 'White', 'Black')\n",
    "\n",
    "# Function to convert 'Evaluation' to 'New_evaluations'\n",
    "def convert_evaluation(row):\n",
    "    eval_str = row['Evaluation']\n",
    "    \n",
    "    if eval_str in ['+M0', '-M0', 'M0']:\n",
    "        return 0.0  # Mate in 0 moves\n",
    "    elif eval_str.startswith('+M') or (eval_str.startswith('M') and not eval_str.startswith('-M')):\n",
    "        return 20.0  # White can mate\n",
    "    elif eval_str.startswith('-M'):\n",
    "        return -20.0  # Black can mate\n",
    "    else:\n",
    "        # Try to convert the evaluation to a float\n",
    "        try:\n",
    "            eval_float = float(eval_str)\n",
    "            return eval_float  # Numeric evaluation remains the same\n",
    "        except ValueError:\n",
    "            return np.nan  # Unable to parse evaluation\n",
    "\n",
    "# Apply the function to create 'New_evaluations' column\n",
    "df['Evaluation'] = df.apply(convert_evaluation, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.0408163265306123,\n",
       " 1.0204081632653061,\n",
       " 96.93877551020408,\n",
       " 98,\n",
       " Outcome\n",
       " Loss    95\n",
       " Win      2\n",
       " Draw     1\n",
       " Name: count, dtype: int64]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Map 'Result' to outcome from White's perspective\n",
    "def get_outcome(result):\n",
    "    if result == '1-0':\n",
    "        return 'Win'    # White won\n",
    "    elif result == '0-1':\n",
    "        return 'Loss'   # White lost\n",
    "    elif result == '1/2-1/2':\n",
    "        return 'Draw'   # Draw\n",
    "    else:\n",
    "        return None     # Exclude other results\n",
    "    \n",
    "    \n",
    "def calculate_chances(df, lower_eval, upper_eval):\n",
    "    # Filter positions where 'New_evaluations' is between lower_eval and upper_eval\n",
    "    positions_in_range = df[(df['Evaluation'] >= lower_eval) & (df['Evaluation'] <= upper_eval)].copy()\n",
    "    \n",
    "    # Get unique GameIDs where this occurs\n",
    "    games_in_range = positions_in_range['GameID'].unique()\n",
    "    \n",
    "    # Get the results of these games\n",
    "    game_results = df[df['GameID'].isin(games_in_range)][['GameID', 'Result']].drop_duplicates()\n",
    "    \n",
    "    # Apply the mapping\n",
    "    game_results['Outcome'] = game_results['Result'].apply(get_outcome)\n",
    "    \n",
    "    # Exclude games with 'Other' outcomes\n",
    "    valid_results = game_results.dropna(subset=['Outcome'])\n",
    "    \n",
    "    # Total number of valid games\n",
    "    total_valid_games = valid_results.shape[0]\n",
    "    outcome_counts=None\n",
    "    if total_valid_games == 0:\n",
    "        winning_chance = drawing_chance = losing_chance = 0.0\n",
    "    else:\n",
    "        # Count the number of games in each category\n",
    "        outcome_counts = valid_results['Outcome'].value_counts()\n",
    "        \n",
    "        # Calculate percentages\n",
    "        winning_chance = (outcome_counts.get('Win', 0) / total_valid_games) * 100\n",
    "        drawing_chance = (outcome_counts.get('Draw', 0) / total_valid_games) * 100\n",
    "        losing_chance = (outcome_counts.get('Loss', 0) / total_valid_games) * 100\n",
    "    \n",
    "    return [winning_chance, drawing_chance, losing_chance, total_valid_games,outcome_counts]\n",
    "\n",
    "\n",
    "calculate_chances(df,-21,-19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winning_chance_table=pd.read_csv(\"winning_chances_adjusted.csv\")\n",
    "intervals = np.arange(-21, 21.5, 0.2)\n",
    "intervals = np.round(intervals, decimals=1)\n",
    "bin_labels = [f\"({intervals[i]}, {intervals[i+1]}]\" for i in range(len(intervals) - 1)]\n",
    "\n",
    "# Bin 'New_evaluations' in 'df' to create an 'Interval' column\n",
    "df['Interval'] = pd.cut(\n",
    "    df['New_evaluations'],\n",
    "    bins=intervals,\n",
    "    labels=bin_labels,\n",
    "    right=True,\n",
    "    include_lowest=True,\n",
    ")\n",
    "\n",
    "# Merge 'df' with 'winning_chance_table' on 'Interval' to get 'WinningChance'\n",
    "df = df.merge(winning_chance_table[['Interval', 'WinningChance', \"LosingChance\"]], on='Interval', how='left')\n",
    "\n",
    "# Rename 'WinningChance' column to 'Winning_Chance' in 'df'\n",
    "df.rename(columns={'WinningChance': 'Winning_Chance'}, inplace=True)\n",
    "\n",
    "\n",
    "#df.to_csv(\"../huge_analyzed_games/combined_analyzed_15_16_winning_chances.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Chances based on Move Order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your 'get_outcome' function\n",
    "def get_outcome(result):\n",
    "    if result == '1-0':\n",
    "        return 'Win'    # White won\n",
    "    elif result == '0-1':\n",
    "        return 'Loss'   # White lost\n",
    "    elif result == '1/2-1/2':\n",
    "        return 'Draw'   # Draw\n",
    "    else:\n",
    "        return None     # Exclude other results\n",
    "    \n",
    "def calculate_chances(df, lower_eval, upper_eval, lower_move, upper_move):\n",
    "    # Filter positions where 'New_evaluations' is between lower_eval and upper_eval\n",
    "    # and 'MoveNumber' is between lower_move and upper_move\n",
    "    positions_in_range = df[\n",
    "        (df['Evaluation'] >= lower_eval) &\n",
    "        (df['Evaluation'] <= upper_eval) &\n",
    "        (df['MoveNumber'] >= lower_move) &\n",
    "        (df['MoveNumber'] <= upper_move)\n",
    "    ].copy()\n",
    "\n",
    "    # Get unique GameIDs where this occurs\n",
    "    games_in_range = positions_in_range['GameID'].unique()\n",
    "\n",
    "    # Get the results of these games, ensuring one entry per GameID\n",
    "    game_results = df[df['GameID'].isin(games_in_range)][['GameID', 'Result']].drop_duplicates(subset='GameID')\n",
    "\n",
    "    # Apply the mapping\n",
    "    game_results['Outcome'] = game_results['Result'].apply(get_outcome)\n",
    "\n",
    "    # Exclude games with 'Other' outcomes\n",
    "    valid_results = game_results.dropna(subset=['Outcome'])\n",
    "\n",
    "    # Total number of valid games\n",
    "    total_valid_games = valid_results.shape[0]\n",
    "\n",
    "    outcome_counts = None\n",
    "    if total_valid_games == 0:\n",
    "        winning_chance = drawing_chance = losing_chance = 0.0\n",
    "    else:\n",
    "        # Count the number of games in each category\n",
    "        outcome_counts = valid_results['Outcome'].value_counts()\n",
    "\n",
    "        # Calculate percentages\n",
    "        winning_chance = (outcome_counts.get('Win', 0) / total_valid_games) * 100\n",
    "        drawing_chance = (outcome_counts.get('Draw', 0) / total_valid_games) * 100\n",
    "        losing_chance = (outcome_counts.get('Loss', 0) / total_valid_games) * 100\n",
    "\n",
    "    return [winning_chance, drawing_chance, losing_chance, total_valid_games, outcome_counts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, create move bins of 5 moves (assuming 'MoveNumber' increments by 1 per half-move)\n",
    "# So each bin will cover 10 half-moves (5 full moves)\n",
    "df=pd.read_csv(\"../huge_analyzed_games/combined_analyzed_games.csv\")\n",
    "# Define the maximum move number\n",
    "max_move_number = df['MoveNumber'].max()\n",
    "\n",
    "# Create move bins\n",
    "move_bins = []\n",
    "for i in range(0, int(max_move_number) + 10, 10):  # Increment by 10 half-moves\n",
    "    lower_move = i + 1  # Start from 1\n",
    "    upper_move = i + 10\n",
    "    move_bins.append((lower_move, upper_move))\n",
    "\n",
    "# Example evaluation interval\n",
    "lower_eval = 4\n",
    "upper_eval = 4.2\n",
    "\n",
    "# Prepare a list to hold the results\n",
    "results = []\n",
    "\n",
    "# Loop over move bins and calculate chances\n",
    "for bin_index, (lower_move, upper_move) in enumerate(move_bins, start=1):\n",
    "    winning_chance, drawing_chance, losing_chance, total_valid_games, _ = calculate_chances(\n",
    "        df, lower_eval, upper_eval, lower_move, upper_move\n",
    "    )\n",
    "    \n",
    "    # Store the results\n",
    "    results.append({\n",
    "        'MoveBin': f\"Bin {bin_index} ({lower_move}-{upper_move})\",\n",
    "        'WinningChance': winning_chance,\n",
    "        'DrawingChance': drawing_chance,\n",
    "        'LosingChance': losing_chance,\n",
    "        'TotalGames': total_valid_games\n",
    "    })\n",
    "\n",
    "# Create a DataFrame from the results\n",
    "winning_chance_table = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the table (no need to run this yourselves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved winning chances table for all moves to winning_chances_all_moves.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming your DataFrame 'df' is loaded and contains the necessary columns\n",
    "\n",
    "# Define the evaluation intervals\n",
    "intervals = np.arange(-13, 13.2, 0.2)\n",
    "intervals = np.round(intervals, decimals=1)\n",
    "edges = [-np.inf] + list(intervals) + [np.inf]\n",
    "\n",
    "# Create bin labels\n",
    "bin_labels = []\n",
    "for i in range(len(edges) - 1):\n",
    "    lower = edges[i]\n",
    "    upper = edges[i + 1]\n",
    "    if np.isneginf(lower):\n",
    "        label = f\"(-infty, {upper}]\"\n",
    "    elif np.isposinf(upper):\n",
    "        label = f\"({lower}, infty)\"\n",
    "    else:\n",
    "        label = f\"({lower}, {upper}]\"\n",
    "    bin_labels.append(label)\n",
    "\n",
    "# Set move range to cover all moves in the dataset\n",
    "lower_move = df['MoveNumber'].min()\n",
    "upper_move = df['MoveNumber'].max()\n",
    "\n",
    "# Prepare a list to hold the results\n",
    "results = []\n",
    "\n",
    "# Loop over evaluation intervals\n",
    "for i in range(len(edges) - 1):\n",
    "    lower_eval = edges[i]\n",
    "    upper_eval = edges[i + 1]\n",
    "\n",
    "    # Call the calculate_chances function with move parameters covering all moves\n",
    "    winning_chance, drawing_chance, losing_chance, total_valid_games, _ = calculate_chances(\n",
    "        df, lower_eval, upper_eval, lower_move, upper_move\n",
    "    )\n",
    "\n",
    "    # Store the results\n",
    "    results.append({\n",
    "        'Interval': bin_labels[i],\n",
    "        'WinningChance': winning_chance,\n",
    "        'DrawingChance': drawing_chance,\n",
    "        'LosingChance': losing_chance,\n",
    "        'TotalGames': total_valid_games,\n",
    "    })\n",
    "\n",
    "# Create a DataFrame from the results\n",
    "winning_chance_table = pd.DataFrame(results)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "output_filename = \"winning_chances_all_moves.csv\"\n",
    "winning_chance_table.to_csv(output_filename, index=False)\n",
    "\n",
    "print(f\"Saved winning chances table for all moves to {output_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_winning_chance_table(df, intervals=np.arange(-13, 13.2, 0.2)):\n",
    "    \"\"\"\n",
    "    Computes the winning, drawing, and losing chances for evaluation intervals.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame containing chess game data with 'Evaluation', 'MoveNumber', 'Result', and 'GameID' columns.\n",
    "    intervals (np.array): Numpy array of interval edges used for binning evaluations.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with winning, drawing, and losing chances per evaluation interval.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    # Round intervals to one decimal place\n",
    "    intervals = np.round(intervals, decimals=1)\n",
    "    edges = [-np.inf] + list(intervals) + [np.inf]\n",
    "\n",
    "    # Create bin labels\n",
    "    bin_labels = []\n",
    "    for i in range(len(edges) - 1):\n",
    "        lower = edges[i]\n",
    "        upper = edges[i + 1]\n",
    "        if np.isneginf(lower):\n",
    "            label = f\"(-∞, {upper}]\"\n",
    "        elif np.isposinf(upper):\n",
    "            label = f\"({lower}, ∞)\"\n",
    "        else:\n",
    "            label = f\"({lower}, {upper}]\"\n",
    "        bin_labels.append(label)\n",
    "\n",
    "    # Set move range to cover all moves in the dataset\n",
    "    lower_move = df['MoveNumber'].min()\n",
    "    upper_move = df['MoveNumber'].max()\n",
    "\n",
    "    # Prepare a list to hold the results\n",
    "    results = []\n",
    "\n",
    "    # Loop over evaluation intervals\n",
    "    for i in range(len(edges) - 1):\n",
    "        lower_eval = edges[i]\n",
    "        upper_eval = edges[i + 1]\n",
    "\n",
    "        # Call the calculate_chances function with move parameters covering all moves\n",
    "        winning_chance, drawing_chance, losing_chance, total_valid_games, _ = calculate_chances(\n",
    "            df, lower_eval, upper_eval, lower_move, upper_move\n",
    "        )\n",
    "\n",
    "        # Store the results\n",
    "        results.append({\n",
    "            'Interval': bin_labels[i],\n",
    "            'WinningChance': winning_chance,\n",
    "            'DrawingChance': drawing_chance,\n",
    "            'LosingChance': losing_chance,\n",
    "            'TotalGames': total_valid_games,\n",
    "        })\n",
    "\n",
    "    # Create a DataFrame from the results\n",
    "    winning_chance_table = pd.DataFrame(results)\n",
    "\n",
    "    return winning_chance_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winning_chance_table=compute_winning_chance_table(df)\n",
    "output_filename = \"winning_chances_all_moves.csv\"\n",
    "winning_chance_table.to_csv(output_filename, index=False)\n",
    "\n",
    "print(f\"Saved winning chances table for all moves to {output_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating The tables based on move order (no need to run this yourselves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "df=pd.read_csv(\"../huge_analyzed_games/combined_analyzed_games.csv\")\n",
    "#df=pd.read_csv(\"../Cleaned_Analyzed_Games/twic1477_16_processed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved winning chances table for moves 1-20 to winning_chances_moves_1-20.csv\n",
      "Saved winning chances table for moves 21-40 to winning_chances_moves_21-40.csv\n",
      "Saved winning chances table for moves 41-60 to winning_chances_moves_41-60.csv\n",
      "Saved winning chances table for moves 61-80 to winning_chances_moves_61-80.csv\n",
      "Saved winning chances table for moves 81-100 to winning_chances_moves_81-100.csv\n",
      "Saved winning chances table for moves 101-120 to winning_chances_moves_101-120.csv\n",
      "Copied winning chances table for moves 121-140 from moves 101-120\n",
      "Copied winning chances table for moves 141-160 from moves 101-120\n",
      "Copied winning chances table for moves 161-180 from moves 101-120\n",
      "Copied winning chances table for moves 181-200 from moves 101-120\n",
      "Copied winning chances table for moves 201-220 from moves 101-120\n",
      "Copied winning chances table for moves 221-240 from moves 101-120\n",
      "Copied winning chances table for moves 241-260 from moves 101-120\n",
      "Copied winning chances table for moves 261-280 from moves 101-120\n",
      "Copied winning chances table for moves 281-300 from moves 101-120\n",
      "Copied winning chances table for moves 301-320 from moves 101-120\n",
      "Copied winning chances table for moves 321-340 from moves 101-120\n",
      "Copied winning chances table for moves 341-360 from moves 101-120\n",
      "Copied winning chances table for moves 361-380 from moves 101-120\n",
      "Copied winning chances table for moves 381-400 from moves 101-120\n",
      "Copied winning chances table for moves 401-420 from moves 101-120\n",
      "Copied winning chances table for moves 421-440 from moves 101-120\n",
      "Copied winning chances table for moves 441-460 from moves 101-120\n",
      "Copied winning chances table for moves 461-480 from moves 101-120\n",
      "Copied winning chances table for moves 481-500 from moves 101-120\n",
      "Copied winning chances table for moves 501-520 from moves 101-120\n",
      "Copied winning chances table for moves 521-540 from moves 101-120\n",
      "Copied winning chances table for moves 541-560 from moves 101-120\n",
      "Copied winning chances table for moves 561-580 from moves 101-120\n",
      "Copied winning chances table for moves 581-600 from moves 101-120\n",
      "Copied winning chances table for moves 601-620 from moves 101-120\n",
      "Copied winning chances table for moves 621-640 from moves 101-120\n"
     ]
    }
   ],
   "source": [
    "output_dir = '../test_winning_chances_tables'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Define the move bins\n",
    "max_move_number = 120\n",
    "move_range = 20\n",
    "move_bins = []\n",
    "for i in range(0, max_move_number, move_range):\n",
    "    lower_move = i + 1\n",
    "    upper_move = i + move_range\n",
    "    move_bins.append((lower_move, upper_move))\n",
    "\n",
    "# Define the evaluation intervals\n",
    "intervals = np.arange(-13, 13.2, 0.2)\n",
    "intervals = np.round(intervals, decimals=1)\n",
    "edges = [-np.inf] + list(intervals) + [np.inf]\n",
    "\n",
    "# Create bin labels\n",
    "bin_labels = []\n",
    "for i in range(len(edges) - 1):\n",
    "    lower = edges[i]\n",
    "    upper = edges[i + 1]\n",
    "    if np.isneginf(lower):\n",
    "        label = f\"(-infty, {upper}]\"\n",
    "    elif np.isposinf(upper):\n",
    "        label = f\"({lower}, infty)\"\n",
    "    else:\n",
    "        label = f\"({lower}, {upper}]\"\n",
    "    bin_labels.append(label)\n",
    "\n",
    "# Initialize variable to hold the reference table for moves 101-120\n",
    "reference_winning_chance_table = None\n",
    "\n",
    "# Loop over move bins up to 120\n",
    "for lower_move, upper_move in move_bins:\n",
    "    # Prepare a list to hold the results\n",
    "    results = []\n",
    "\n",
    "    # Loop over evaluation intervals\n",
    "    for i in range(len(edges) - 1):\n",
    "        lower_eval = edges[i]\n",
    "        upper_eval = edges[i + 1]\n",
    "\n",
    "        # Call the calculate_chances function with move bin parameters\n",
    "        winning_chance, drawing_chance, losing_chance, total_valid_games, _ = calculate_chances(\n",
    "            df, lower_eval, upper_eval, lower_move, upper_move\n",
    "        )\n",
    "\n",
    "        # Store the results\n",
    "        results.append({\n",
    "            'Interval': bin_labels[i],\n",
    "            'WinningChance': winning_chance,\n",
    "            'DrawingChance': drawing_chance,\n",
    "            'LosingChance': losing_chance,\n",
    "            'TotalGames': total_valid_games,\n",
    "        })\n",
    "\n",
    "    # Create a DataFrame from the results\n",
    "    winning_chance_table = pd.DataFrame(results)\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    move_bin_label = f\"{lower_move}-{upper_move}\"\n",
    "    output_filename = f\"winning_chances_moves_{move_bin_label}.csv\"\n",
    "    output_path = os.path.join(output_dir, output_filename)\n",
    "    winning_chance_table.to_csv(output_path, index=False)\n",
    "\n",
    "    print(f\"Saved winning chances table for moves {lower_move}-{upper_move} to {output_filename}\")\n",
    "\n",
    "    # If this is the move bin for moves 101-120, save the winning_chance_table as the reference table\n",
    "    if lower_move == 101 and upper_move == 120:\n",
    "        reference_winning_chance_table = winning_chance_table.copy()\n",
    "\n",
    "# For move bins beyond 120, copy the table from moves 101-120\n",
    "max_move_number_in_df = df['MoveNumber'].max()\n",
    "additional_move_bins = []\n",
    "current_move = 120\n",
    "while current_move < max_move_number_in_df:\n",
    "    lower_move = current_move + 1\n",
    "    upper_move = current_move + move_range\n",
    "    additional_move_bins.append((lower_move, upper_move))\n",
    "    current_move += move_range\n",
    "\n",
    "# Copy the reference table to additional move bins\n",
    "if reference_winning_chance_table is not None:\n",
    "    for lower_move, upper_move in additional_move_bins:\n",
    "        # Save the reference DataFrame to a CSV file with the new move bin label\n",
    "        move_bin_label = f\"{lower_move}-{upper_move}\"\n",
    "        output_filename = f\"winning_chances_moves_{move_bin_label}.csv\"\n",
    "        output_path = os.path.join(output_dir, output_filename)\n",
    "        reference_winning_chance_table.to_csv(output_path, index=False)\n",
    "        print(f\"Copied winning chances table for moves {lower_move}-{upper_move} from moves 101-120\")\n",
    "else:\n",
    "    print(\"Reference table for moves 101-120 is not available. Cannot copy to later move bins.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1174923"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.GameID.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Winning Chances Column based on multiple tables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "#df=pd.read_csv(\"../huge_analyzed_games/combined_analyzed_games.csv\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "df= pd.read_csv(\"../Cleaned_Analyzed_Games/twic920_15_processed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Assume 'df' is your DataFrame with the analyzed games\n",
    "\n",
    "# Read and combine all winning chances tables\n",
    "winning_chances_tables = []\n",
    "for filename in glob.glob(\"../winning_chances_tables/winning_chances_moves_*-*.csv\"):\n",
    "    # Extract the move bin from the filename\n",
    "    basename = os.path.basename(filename)\n",
    "    match = re.match(r'winning_chances_moves_(\\d+)-(\\d+)\\.csv', basename)\n",
    "    if match:\n",
    "        lower_move = int(match.group(1))\n",
    "        upper_move = int(match.group(2))\n",
    "        move_bin_label = f\"{lower_move}-{upper_move}\"\n",
    "        # Read the table\n",
    "        winning_chance_table = pd.read_csv(filename)\n",
    "        # Add the 'MoveBin' column\n",
    "        winning_chance_table['MoveBin'] = move_bin_label\n",
    "        # Append to the list\n",
    "        winning_chances_tables.append(winning_chance_table)\n",
    "    else:\n",
    "        print(f\"Filename '{basename}' does not match the expected pattern.\")\n",
    "\n",
    "# Combine all the winning chances tables\n",
    "winning_chances_combined = pd.concat(winning_chances_tables, ignore_index=True)\n",
    "\n",
    "# Prepare the main DataFrame 'df'\n",
    "\n",
    "# Ensure 'MoveNumber' is numeric\n",
    "df['MoveNumber'] = pd.to_numeric(df['MoveNumber'], errors='coerce')\n",
    "\n",
    "# Define the move bins as in the winning chances tables\n",
    "max_move_number = df['MoveNumber'].max()\n",
    "bin_size = 20  # 20 half-moves per bin, as per your move range\n",
    "\n",
    "# Create edges for the bins\n",
    "edges = list(range(1, int(max_move_number) + bin_size + 1, bin_size))\n",
    "labels = [f\"{edges[i]}-{edges[i+1]-1}\" for i in range(len(edges)-1)]\n",
    "\n",
    "# Assign 'MoveBin' labels to 'MoveNumber' in df\n",
    "df['MoveBin'] = pd.cut(\n",
    "    df['MoveNumber'],\n",
    "    bins=edges,\n",
    "    labels=labels,\n",
    "    right=False,\n",
    "    include_lowest=True\n",
    ")\n",
    "\n",
    "# Define evaluation intervals matching those in the winning chances tables\n",
    "intervals = np.arange(-13, 13.2, 0.2)\n",
    "intervals = np.round(intervals, decimals=1)\n",
    "edges = [-np.inf] + list(intervals) + [np.inf]\n",
    "\n",
    "# Create bin labels\n",
    "bin_labels = []\n",
    "for i in range(len(edges) -1):\n",
    "    lower = edges[i]\n",
    "    upper = edges[i+1]\n",
    "    if np.isneginf(lower):\n",
    "        label = f\"(-infty, {upper}]\"\n",
    "    elif np.isposinf(upper):\n",
    "        label = f\"({lower}, infty)\"\n",
    "    else:\n",
    "        label = f\"({lower}, {upper}]\"\n",
    "    bin_labels.append(label)\n",
    "\n",
    "# Bin 'Evaluation' in 'df' to create an 'Interval' column\n",
    "df['Interval'] = pd.cut(\n",
    "    df['Evaluation'],\n",
    "    bins=edges,\n",
    "    labels=bin_labels,\n",
    "    right=True,\n",
    "    include_lowest=True,\n",
    ")\n",
    "\n",
    "# Ensure 'MoveBin' and 'Interval' are strings for consistent merging\n",
    "df['MoveBin'] = df['MoveBin'].astype(str)\n",
    "df['Interval'] = df['Interval'].astype(str)\n",
    "winning_chances_combined['MoveBin'] = winning_chances_combined['MoveBin'].astype(str)\n",
    "winning_chances_combined['Interval'] = winning_chances_combined['Interval'].astype(str)\n",
    "\n",
    "# Merge 'df' with 'winning_chances_combined' on 'MoveBin' and 'Interval'\n",
    "df = df.merge(\n",
    "    winning_chances_combined[['MoveBin', 'Interval', 'WinningChance', 'LosingChance', 'TotalGames']],\n",
    "    on=['MoveBin', 'Interval'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Now 'df' has the 'WinningChance', 'LosingChance', and 'TotalGames' columns added\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['WCL'] = df.groupby('GameID')['WinningChance'].diff().abs()\n",
    "df['LCL'] = df.groupby('GameID')['LosingChance'].diff().abs()\n",
    "df.loc[df['MoveNumber'] % 2 == 0, 'WCL'] = None\n",
    "df.loc[df['MoveNumber'] % 2 != 0, 'LCL'] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Winning Chances Column based on ONE table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "pd.set_option('display.max_columns', None)\n",
    "#df=pd.read_csv(\"../huge_analyzed_games/combined_analyzed_games.csv\")\n",
    "#df= pd.read_csv(\"../Cleaned_Analyzed_Games/twic920_15_processed.csv\")\n",
    "df=pd.read_csv(\"../huge_analyzed_games/combined_analyzed_games_20.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_chess_data(df, winning_chance_table=pd.read_csv('winning_chances_all_moves.csv'), intervals=np.arange(-13, 13.2, 0.2)):\n",
    "    \"\"\"\n",
    "    Processes chess data by binning evaluation values, merging with winning chances,\n",
    "    and computing WCL, LCL, Player, and 'a' columns.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame containing chess game data with 'Evaluation', 'GameID', and 'MoveNumber' columns.\n",
    "    winning_chance_table (pd.DataFrame): DataFrame containing winning chances with 'Interval', 'WinningChance', 'LosingChance', and 'TotalGames' columns.\n",
    "    intervals (np.array): Numpy array of interval edges used for binning evaluations.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Modified DataFrame with additional columns added.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    # Ensure intervals are rounded to one decimal place\n",
    "    intervals = np.round(intervals, decimals=1)\n",
    "    edges = [-np.inf] + list(intervals) + [np.inf]\n",
    "\n",
    "    # Create bin labels\n",
    "    bin_labels = []\n",
    "    for i in range(len(edges) - 1):\n",
    "        lower = edges[i]\n",
    "        upper = edges[i + 1]\n",
    "        if np.isneginf(lower):\n",
    "            label = f\"(-infty, {upper}]\"\n",
    "        elif np.isposinf(upper):\n",
    "            label = f\"({lower}, infty)\"\n",
    "        else:\n",
    "            label = f\"({lower}, {upper}]\"\n",
    "        bin_labels.append(label)\n",
    "\n",
    "    # Ensure that the bin labels in 'winning_chance_table' match the ones we're creating\n",
    "    # This is important for a correct merge\n",
    "    winning_chance_table['Interval'] = winning_chance_table['Interval'].astype(str)\n",
    "    bin_labels = [str(label) for label in bin_labels]\n",
    "\n",
    "    # Bin the 'Evaluation' values in 'df' to create an 'Interval' column\n",
    "    df['Interval'] = pd.cut(\n",
    "        df['Evaluation'],\n",
    "        bins=edges,\n",
    "        labels=bin_labels,\n",
    "        right=True,\n",
    "        include_lowest=True,\n",
    "    )\n",
    "\n",
    "    # Ensure 'Interval' in 'df' is of type string\n",
    "    df['Interval'] = df['Interval'].astype(str)\n",
    "\n",
    "    # Select the columns to merge\n",
    "    columns_to_merge = ['Interval', 'WinningChance', 'LosingChance', 'TotalGames']\n",
    "\n",
    "    # Merge 'df' with 'winning_chance_table' on 'Interval'\n",
    "    df = df.merge(\n",
    "        winning_chance_table[columns_to_merge],\n",
    "        on='Interval',\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # Compute 'WCL' and 'LCL' differences per game\n",
    "    df['WCL'] = df.groupby('GameID')['WinningChance'].diff().abs()\n",
    "    df['LCL'] = df.groupby('GameID')['LosingChance'].diff().abs()\n",
    "\n",
    "    # Assign 'Player' based on move number\n",
    "    df['Player'] = np.where(df['MoveNumber'] % 2 != 0, 'White', 'Black')\n",
    "\n",
    "    # Compute 'a' = max(|WCL|, |LCL|) for each move\n",
    "    df['a'] = df[['WCL', 'LCL']].abs().max(axis=1)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "pd.set_option('display.max_columns', None)\n",
    "#df=pd.read_csv(\"../huge_analyzed_games/combined_analyzed_games.csv\")\n",
    "#df= pd.read_csv(\"../Cleaned_Analyzed_Games/twic920_15_processed.csv\")\n",
    "df=pd.read_csv(\"../huge_analyzed_games/combined_analyzed_games_20.csv\")\n",
    "df=process_chess_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_summary_table(df, mistake_bins= [5, 10, 15, 20, 25, 30, 35, 40, 50, 60, 70, 100], winning_chance_table=None, intervals=None):\n",
    "    \"\"\"\n",
    "    Processes the chess DataFrame to create a summary table of mistakes per interval per player per game.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame containing chess game data.\n",
    "    mistake_bins (list): List of bin edges for mistake intervals.\n",
    "    winning_chance_table (pd.DataFrame, optional): DataFrame containing winning chances.\n",
    "                                         Default is loaded from 'winning_chances_all_moves.csv'.\n",
    "    intervals (np.array, optional): Numpy array of interval edges used for binning evaluations.\n",
    "                                    Default is np.arange(-13, 13.2, 0.2).\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Summary table with mistakes per interval per player per game.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    # If 'WCL' does not exist, apply the 'process_chess_data' function\n",
    "    if 'WCL' not in df.columns or 'a' not in df.columns:\n",
    "        if winning_chance_table is None:\n",
    "            winning_chance_table = pd.read_csv('winning_chances_all_moves.csv')\n",
    "        if intervals is None:\n",
    "            intervals = np.arange(-13, 13.2, 0.2)\n",
    "        df = process_chess_data(df, winning_chance_table, intervals)\n",
    "\n",
    "    # Step 1: Define mistake labels based on mistake_bins\n",
    "    mistake_labels = []\n",
    "    for i in range(len(mistake_bins)-1):\n",
    "        label = f'({mistake_bins[i]},{mistake_bins[i+1]}]'\n",
    "        mistake_labels.append(label)\n",
    "\n",
    "    # Step 2: Assign each 'a' to a mistake interval\n",
    "    df['MistakeInterval'] = pd.cut(\n",
    "        df['a'],\n",
    "        bins=mistake_bins,\n",
    "        labels=mistake_labels,\n",
    "        right=True,\n",
    "        include_lowest=True\n",
    "    )\n",
    "\n",
    "    # Step 3: Identify the player making the move if 'Player' column doesn't exist\n",
    "    if 'Player' not in df.columns:\n",
    "        df['Player'] = np.where(df['MoveNumber'] % 2 != 0, 'White', 'Black')\n",
    "\n",
    "    # Step 4: Group and count the number of mistakes per interval, per player, per game\n",
    "    mistake_moves = df.dropna(subset=['MistakeInterval'])\n",
    "    mistake_counts = mistake_moves.groupby(['GameID', 'Player', 'MistakeInterval']).size().reset_index(name='MistakeCount')\n",
    "\n",
    "    # Step 5: Pivot the data to get a summary table per game and player\n",
    "    summary_table = mistake_counts.pivot_table(\n",
    "        index=['GameID', 'Player'],\n",
    "        columns='MistakeInterval',\n",
    "        values='MistakeCount',\n",
    "        fill_value=0\n",
    "    ).reset_index()\n",
    "\n",
    "    # Flatten the column MultiIndex if necessary\n",
    "    summary_table.columns.name = None\n",
    "    summary_table.columns = [col if isinstance(col, str) else col for col in summary_table.columns]\n",
    "\n",
    "    # Step 6: Compute Total Moves per game\n",
    "    total_moves = df.groupby('GameID')['MoveNumber'].max().reset_index(name='TotalMoves')\n",
    "\n",
    "    # Step 7: Extract game-level metadata: Opening, Variation, Result\n",
    "    game_metadata = df.groupby('GameID').agg({\n",
    "        'Opening': 'first',\n",
    "        'Variation': 'first',\n",
    "        'Result': 'first'\n",
    "    }).reset_index()\n",
    "\n",
    "    # Merge TotalMoves into game_metadata\n",
    "    game_metadata = game_metadata.merge(total_moves, on='GameID', how='left')\n",
    "\n",
    "    # Step 8: Extract player-level metadata\n",
    "    player_metadata = df.groupby('GameID').agg({\n",
    "        'WhiteName': 'first',\n",
    "        'WhiteElo': 'first',\n",
    "        'WhiteFideId': 'first',\n",
    "        'BlackName': 'first',\n",
    "        'BlackElo': 'first',\n",
    "        'BlackFideId': 'first'\n",
    "    }).reset_index()\n",
    "\n",
    "    # Prepare player metadata for merging\n",
    "    # For White players\n",
    "    white_players = player_metadata[['GameID', 'WhiteName', 'WhiteElo', 'WhiteFideId']].copy()\n",
    "    white_players['Player'] = 'White'\n",
    "    white_players = white_players.rename(columns={\n",
    "        'WhiteName': 'Name',\n",
    "        'WhiteElo': 'Elo',\n",
    "        'WhiteFideId': 'FideId'\n",
    "    })\n",
    "\n",
    "    # For Black players\n",
    "    black_players = player_metadata[['GameID', 'BlackName', 'BlackElo', 'BlackFideId']].copy()\n",
    "    black_players['Player'] = 'Black'\n",
    "    black_players = black_players.rename(columns={\n",
    "        'BlackName': 'Name',\n",
    "        'BlackElo': 'Elo',\n",
    "        'BlackFideId': 'FideId'\n",
    "    })\n",
    "\n",
    "    # Concatenate player metadata\n",
    "    player_metadata_long = pd.concat([white_players, black_players], ignore_index=True)\n",
    "\n",
    "    # Step 9: Merge player metadata with the summary table\n",
    "    summary_table = summary_table.merge(player_metadata_long, on=['GameID', 'Player'], how='left')\n",
    "\n",
    "    # Step 10: Merge game metadata with the summary table\n",
    "    summary_table = summary_table.merge(game_metadata, on='GameID', how='left')\n",
    "    total_moves_bins = [0, 30, 40, 50, 60, 70, 80, 90, 100, 120, np.inf]\n",
    "    total_moves_labels = [\n",
    "        '(0,30]', '(30,40]', '(40,50]', '(50,60]', '(60,70]',\n",
    "        '(70,80]', '(80,90]', '(90,100]', '(100,120]', '(120,∞)'\n",
    "    ]\n",
    "\n",
    "    # Step 2: Assign each game to a TotalMovesInterval\n",
    "    summary_table['TotalMovesInterval'] = pd.cut(\n",
    "        summary_table['TotalMoves'],\n",
    "        bins=total_moves_bins,\n",
    "        labels=total_moves_labels,\n",
    "        right=True,\n",
    "        include_lowest=True\n",
    "    )\n",
    "    # Rearranging columns for better readability\n",
    "    cols = ['GameID', 'Player', 'Name', 'Elo', 'FideId', 'Opening', 'Variation', 'Result', 'TotalMoves', 'TotalMovesInterval'] + mistake_labels\n",
    "    summary_table = summary_table[cols]\n",
    "\n",
    "    return summary_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\foivo\\AppData\\Local\\Temp\\ipykernel_67068\\521829234.py:48: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  mistake_counts = mistake_moves.groupby(['GameID', 'Player', 'MistakeInterval']).size().reset_index(name='MistakeCount')\n",
      "C:\\Users\\foivo\\AppData\\Local\\Temp\\ipykernel_67068\\521829234.py:51: FutureWarning: The default value of observed=False is deprecated and will change to observed=True in a future version of pandas. Specify observed=False to silence this warning and retain the current behavior\n",
      "  summary_table = mistake_counts.pivot_table(\n"
     ]
    }
   ],
   "source": [
    "summary_table=create_summary_table(df)\n",
    "#summary_table.to_csv(\"../huge_analyzed_games/big_summary_table.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\foivo\\AppData\\Local\\Temp\\ipykernel_85248\\521829234.py:48: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  mistake_counts = mistake_moves.groupby(['GameID', 'Player', 'MistakeInterval']).size().reset_index(name='MistakeCount')\n",
      "C:\\Users\\foivo\\AppData\\Local\\Temp\\ipykernel_85248\\521829234.py:51: FutureWarning: The default value of observed=False is deprecated and will change to observed=True in a future version of pandas. Specify observed=False to silence this warning and retain the current behavior\n",
      "  summary_table = mistake_counts.pivot_table(\n"
     ]
    }
   ],
   "source": [
    "table=create_summary_table(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "summary_table=pd.read_csv(\"../huge_analyzed_games/big_summary_table.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistake_bins= [5, 10, 15, 20, 25, 30, 35, 40, 50, 60, 70, 100]\n",
    "mistake_labels = []\n",
    "for i in range(len(mistake_bins)-1):\n",
    "    label = f'({mistake_bins[i]},{mistake_bins[i+1]}]'\n",
    "    mistake_labels.append(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\foivo\\AppData\\Local\\Temp\\ipykernel_67068\\3311506717.py:49: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if not pd.api.types.is_categorical_dtype(summary_table[col]):\n",
      "C:\\Users\\foivo\\AppData\\Local\\Temp\\ipykernel_67068\\3311506717.py:49: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if not pd.api.types.is_categorical_dtype(summary_table[col]):\n",
      "C:\\Users\\foivo\\AppData\\Local\\Temp\\ipykernel_67068\\3311506717.py:49: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if not pd.api.types.is_categorical_dtype(summary_table[col]):\n",
      "C:\\Users\\foivo\\AppData\\Local\\Temp\\ipykernel_67068\\3311506717.py:49: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if not pd.api.types.is_categorical_dtype(summary_table[col]):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE): 265.69\n",
      "R-squared Score (R²): 0.12\n",
      "Percentage of predictions within ±300 Elo: 75.97%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load your summary_table\n",
    "#summary_table = pd.read_csv(\"../huge_analyzed_games/big_summary_table.csv\")\n",
    "\n",
    "# Define the mistake intervals and labels\n",
    "mistake_bins = [5, 10, 15, 20, 25, 30, 35, 40, 50, 60, 70, 100]\n",
    "mistake_labels = []\n",
    "for i in range(len(mistake_bins)-1):\n",
    "    label = f'({mistake_bins[i]},{mistake_bins[i+1]}]'\n",
    "    mistake_labels.append(label)\n",
    "\n",
    "# Ensure 'TotalMovesInterval' is added to summary_table\n",
    "total_moves_bins = [0, 30, 40, 50, 60, 70, 80, 90, 100, 120, np.inf]\n",
    "total_moves_labels = []\n",
    "for i in range(len(total_moves_bins)-1):\n",
    "    lower = total_moves_bins[i]\n",
    "    upper = total_moves_bins[i+1]\n",
    "    if np.isinf(upper):\n",
    "        label = f'({lower},∞]'\n",
    "    else:\n",
    "        label = f'({lower},{upper}]'\n",
    "    total_moves_labels.append(label)\n",
    "    \n",
    "summary_table['TotalMovesInterval'] = pd.cut(\n",
    "    summary_table['TotalMoves'],\n",
    "    bins=total_moves_bins,\n",
    "    labels=total_moves_labels,\n",
    "    right=True,\n",
    "    include_lowest=True\n",
    ")\n",
    "\n",
    "# Handle missing values in the target variable\n",
    "summary_table = summary_table.dropna(subset=['Elo'])\n",
    "\n",
    "# Handle missing values in categorical features by adding 'Unknown' to categories\n",
    "categorical_features = ['Opening', 'Variation', 'Result', 'TotalMovesInterval', 'Player']\n",
    "\n",
    "for col in categorical_features:\n",
    "    # Ensure the column is of 'category' dtype\n",
    "    if not pd.api.types.is_categorical_dtype(summary_table[col]):\n",
    "        summary_table[col] = summary_table[col].astype('category')\n",
    "    # Add 'Unknown' to categories if not already present\n",
    "    if 'Unknown' not in summary_table[col].cat.categories:\n",
    "        summary_table[col] = summary_table[col].cat.add_categories(['Unknown'])\n",
    "    # Fill NaN values with 'Unknown'\n",
    "    summary_table[col] = summary_table[col].fillna('Unknown')\n",
    "\n",
    "# Handle missing values in numerical features (mistake intervals)\n",
    "mistake_intervals = mistake_labels  # List of mistake interval columns\n",
    "summary_table[mistake_intervals] = summary_table[mistake_intervals].fillna(0)\n",
    "\n",
    "# Define target and features\n",
    "y = summary_table['Elo']\n",
    "\n",
    "# Define the features\n",
    "feature_columns = ['Opening',  'Result', 'TotalMovesInterval', ] + mistake_intervals\n",
    "X = summary_table[feature_columns]\n",
    "\n",
    "# Identify categorical and numerical features\n",
    "categorical_features = ['Opening', 'Result', 'TotalMovesInterval', ]\n",
    "numerical_features = mistake_intervals\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create a ColumnTransformer to apply OneHotEncoder to categorical features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough'  # Pass through numerical features without changes\n",
    ")\n",
    "\n",
    "# Create a pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'Root Mean Squared Error (RMSE): {rmse:.2f}')\n",
    "print(f'R-squared Score (R²): {r2:.2f}')\n",
    "\n",
    "absolute_errors = np.abs(y_pred - y_test)\n",
    "threshold = 300\n",
    "within_threshold = np.sum(absolute_errors <= threshold)\n",
    "total_predictions = len(y_test)\n",
    "percentage_within_threshold = (within_threshold / total_predictions) * 100\n",
    "\n",
    "print(f\"Percentage of predictions within ±{threshold} Elo: {percentage_within_threshold:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\foivo\\AppData\\Local\\Temp\\ipykernel_85248\\3384362917.py:18: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if not pd.api.types.is_categorical_dtype(player_games[col]):\n"
     ]
    }
   ],
   "source": [
    "# Specify the player's name or FIDE ID\n",
    "player_name = 'Caruana Fabiano'  # Replace with the player's name\n",
    "player_fide_id = 2020009       # Replace with the player's FIDE ID (if available)\n",
    "\n",
    "# Extract the 5 games for the player\n",
    "player_games = summary_table[\n",
    "    (summary_table['Name'] == player_name) | (summary_table['FideId'] == player_fide_id)\n",
    "].head(10)  # Get the first 5 games\n",
    "\n",
    "# If you have specific GameIDs\n",
    "#game_ids = [1, 2, 3, 4, 5]  # Replace with the actual GameIDs\n",
    "#player_games = summary_table[summary_table['GameID'].isin(game_ids)]\n",
    "# Handle missing values in categorical features\n",
    "categorical_features = ['Opening', 'Variation', 'Result', 'TotalMovesInterval', 'Player']\n",
    "\n",
    "for col in categorical_features:\n",
    "    # Ensure the column is of 'category' dtype\n",
    "    if not pd.api.types.is_categorical_dtype(player_games[col]):\n",
    "        player_games[col] = player_games[col].astype('category')\n",
    "    # Add 'Unknown' to categories if not already present\n",
    "    if 'Unknown' not in player_games[col].cat.categories:\n",
    "        player_games[col] = player_games[col].cat.add_categories(['Unknown'])\n",
    "    # Fill NaN values with 'Unknown'\n",
    "    player_games[col] = player_games[col].fillna('Unknown')\n",
    "\n",
    "# Handle missing values in numerical features (mistake intervals)\n",
    "player_games[mistake_intervals] = player_games[mistake_intervals].fillna(0)\n",
    "\n",
    "# Define the features\n",
    "feature_columns = ['Opening', 'Variation', 'Result', 'TotalMovesInterval', 'Player'] + mistake_intervals\n",
    "X_player = player_games[feature_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Elo for Game 1: 2258.58\n",
      "Predicted Elo for Game 2: 2216.70\n",
      "Predicted Elo for Game 3: 2371.31\n",
      "Predicted Elo for Game 4: 2376.94\n",
      "Predicted Elo for Game 5: 2287.00\n",
      "Predicted Elo for Game 6: 2467.72\n",
      "Predicted Elo for Game 7: 2269.58\n",
      "Predicted Elo for Game 8: 2402.88\n",
      "Predicted Elo for Game 9: 2242.95\n",
      "Predicted Elo for Game 10: 2418.32\n",
      "\n",
      "Average Predicted Elo for Caruana Fabiano: 2331.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\foivo\\AppData\\Local\\Temp\\ipykernel_85248\\587716437.py:6: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if not pd.api.types.is_categorical_dtype(player_games[col]):\n"
     ]
    }
   ],
   "source": [
    "# Handle missing values in categorical features\n",
    "categorical_features = ['Opening', 'Variation', 'Result', 'TotalMovesInterval', 'Player']\n",
    "\n",
    "for col in categorical_features:\n",
    "    # Ensure the column is of 'category' dtype\n",
    "    if not pd.api.types.is_categorical_dtype(player_games[col]):\n",
    "        player_games[col] = player_games[col].astype('category')\n",
    "    # Add 'Unknown' to categories if not already present\n",
    "    if 'Unknown' not in player_games[col].cat.categories:\n",
    "        player_games[col] = player_games[col].cat.add_categories(['Unknown'])\n",
    "    # Fill NaN values with 'Unknown'\n",
    "    player_games[col] = player_games[col].fillna('Unknown')\n",
    "\n",
    "# Handle missing values in numerical features (mistake intervals)\n",
    "player_games[mistake_intervals] = player_games[mistake_intervals].fillna(0)\n",
    "\n",
    "# Define the features\n",
    "feature_columns = ['Opening', 'Variation', 'Result', 'TotalMovesInterval', 'Player'] + mistake_intervals\n",
    "X_player = player_games[feature_columns]\n",
    "\n",
    "y_player_pred = pipeline.predict(X_player)\n",
    "\n",
    "# Print the predictions for each game\n",
    "for i, pred in enumerate(y_player_pred):\n",
    "    print(f\"Predicted Elo for Game {i+1}: {pred:.2f}\")\n",
    "    \n",
    "average_predicted_elo = y_player_pred.mean()\n",
    "print(f\"\\nAverage Predicted Elo for {player_name}: {average_predicted_elo:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\foivo\\AppData\\Local\\Temp\\ipykernel_21700\\926839676.py:46: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if not pd.api.types.is_categorical_dtype(summary_table[col]):\n",
      "C:\\Users\\foivo\\AppData\\Local\\Temp\\ipykernel_21700\\926839676.py:46: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if not pd.api.types.is_categorical_dtype(summary_table[col]):\n",
      "C:\\Users\\foivo\\AppData\\Local\\Temp\\ipykernel_21700\\926839676.py:46: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if not pd.api.types.is_categorical_dtype(summary_table[col]):\n",
      "C:\\Users\\foivo\\AppData\\Local\\Temp\\ipykernel_21700\\926839676.py:46: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if not pd.api.types.is_categorical_dtype(summary_table[col]):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE): 269.73\n",
      "R-squared Score (R²): 0.08\n",
      "\n",
      "Top 20 features by absolute coefficient value:\n",
      "                                               Feature  Coefficient\n",
      "254                             Opening_Scotch opening  -451.073717\n",
      "130    Opening_King's Indian Defense: Kazakh Variation  -415.913072\n",
      "36                              Opening_Canard opening  -372.655133\n",
      "282  Opening_Trompowsky Attack: Classical Defense, ...   357.048160\n",
      "112                   Opening_Gruenfeld with e3    Bd3  -334.868810\n",
      "206                             Opening_Queen's Gambit  -328.209702\n",
      "270  Opening_Sicilian, Szen variation, Dely-Kasparo...  -319.903968\n",
      "52                           Opening_Damiano's defence  -317.661726\n",
      "40                           Opening_Caro-Masi defence  -314.348275\n",
      "181          Opening_Pirc Defense: Classical Variation   286.606038\n",
      "293          Opening_Vienna gambit, Steinitz variation   266.532981\n",
      "294             Opening_Vienna gambit, Wurzburger trap  -256.344930\n",
      "31        Opening_Bogo-Indian defence, Monticelli trap   247.059421\n",
      "207  Opening_Queen's Gambit Declined: Exchange Vari...   244.824753\n",
      "237  Opening_Ruy Lopez: Morphy Defense, Arkhangelsk...   244.053123\n",
      "239         Opening_Ruy Lopez: Open, Classical Defense   240.549104\n",
      "43                     Opening_Catalan Opening: Closed   240.087293\n",
      "137                     Opening_King's knight's gambit  -235.160078\n",
      "263  Opening_Sicilian Defense: Nyezhmetdinov-Rossol...  -234.867064\n",
      "262  Opening_Sicilian Defense: Dragon Variation, Fi...   232.882765\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load your summary_table\n",
    "summary_table = pd.read_csv(\"../huge_analyzed_games/big_summary_table.csv\")\n",
    "\n",
    "# Filter data for White players only\n",
    "summary_table = summary_table[summary_table['Player'] == 'White']\n",
    "\n",
    "# Define the mistake intervals and labels\n",
    "mistake_labels = [\n",
    "    '(5,10]', '(10,15]', '(15,20]', '(20,25]', '(25,30]',\n",
    "    '(30,35]', '(35,40]', '(40,50]', '(50,60]', '(60,70]', '(70,100]'\n",
    "]\n",
    "mistake_intervals = mistake_labels  # For clarity\n",
    "\n",
    "# Ensure 'TotalMovesInterval' is added to summary_table\n",
    "total_moves_bins = [0, 30, 40, 50, 60, 70, 80, 90, 100, 120, np.inf]\n",
    "total_moves_labels = [\n",
    "    '(0,30]', '(30,40]', '(40,50]', '(50,60]', '(60,70]',\n",
    "    '(70,80]', '(80,90]', '(90,100]', '(100,120]', '(120,∞)'\n",
    "]\n",
    "summary_table['TotalMovesInterval'] = pd.cut(\n",
    "    summary_table['TotalMoves'],\n",
    "    bins=total_moves_bins,\n",
    "    labels=total_moves_labels,\n",
    "    right=True,\n",
    "    include_lowest=True\n",
    ")\n",
    "\n",
    "# Handle missing values in the target variable\n",
    "summary_table = summary_table.dropna(subset=['Elo'])\n",
    "\n",
    "# Handle missing values in categorical features by adding 'Unknown' to categories\n",
    "categorical_features = ['Opening', 'Variation', 'Result', 'TotalMovesInterval']\n",
    "\n",
    "for col in categorical_features:\n",
    "    # Ensure the column is of 'category' dtype\n",
    "    if not pd.api.types.is_categorical_dtype(summary_table[col]):\n",
    "        summary_table[col] = summary_table[col].astype('category')\n",
    "    # Add 'Unknown' to categories if not already present\n",
    "    if 'Unknown' not in summary_table[col].cat.categories:\n",
    "        summary_table[col] = summary_table[col].cat.add_categories(['Unknown'])\n",
    "    # Fill NaN values with 'Unknown'\n",
    "    summary_table[col] = summary_table[col].fillna('Unknown')\n",
    "\n",
    "# Handle missing values in numerical features (mistake intervals)\n",
    "summary_table[mistake_intervals] = summary_table[mistake_intervals].fillna(0)\n",
    "\n",
    "# Define target and features\n",
    "y = summary_table['Elo']  # This should be the Elo rating of the White player\n",
    "\n",
    "# Define the features\n",
    "feature_columns = [   'Opening', 'Result' ,'TotalMovesInterval'] + mistake_intervals\n",
    "X = summary_table[feature_columns]\n",
    "\n",
    "# Identify categorical and numerical features\n",
    "categorical_features = [   'Opening', 'Result' ,'TotalMovesInterval']\n",
    "numerical_features = mistake_intervals\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create a ColumnTransformer to apply OneHotEncoder to categorical features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough'  # Pass through numerical features without changes\n",
    ")\n",
    "\n",
    "# Create a pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'Root Mean Squared Error (RMSE): {rmse:.2f}')\n",
    "print(f'R-squared Score (R²): {r2:.2f}')\n",
    "\n",
    "# Get the names of the categorical features after one-hot encoding\n",
    "onehot_feature_names = pipeline.named_steps['preprocessor'].named_transformers_['cat'].get_feature_names_out(categorical_features)\n",
    "\n",
    "# Combine with numerical feature names\n",
    "all_feature_names = np.concatenate([onehot_feature_names, numerical_features])\n",
    "\n",
    "# Get the coefficients from the linear regression model\n",
    "coefficients = pipeline.named_steps['regressor'].coef_\n",
    "\n",
    "# Create a DataFrame to display feature names and their coefficients\n",
    "coef_df = pd.DataFrame({\n",
    "    'Feature': all_feature_names,\n",
    "    'Coefficient': coefficients\n",
    "})\n",
    "\n",
    "# Sort the coefficients by absolute value\n",
    "coef_df['AbsCoefficient'] = coef_df['Coefficient'].abs()\n",
    "coef_df = coef_df.sort_values(by='AbsCoefficient', ascending=False)\n",
    "\n",
    "# Display the top 20 features with the highest absolute coefficients\n",
    "print(\"\\nTop 20 features by absolute coefficient value:\")\n",
    "print(coef_df[['Feature', 'Coefficient']].head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE): 266.19\n",
      "R-squared Score (R²): 0.11\n",
      "Percentage of predictions within ±300 Elo: 77.62%\n"
     ]
    }
   ],
   "source": [
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'Root Mean Squared Error (RMSE): {rmse:.2f}')\n",
    "print(f'R-squared Score (R²): {r2:.2f}')\n",
    "\n",
    "# Calculate the percentage of predictions within ±300 Elo\n",
    "absolute_errors = np.abs(y_pred - y_test)\n",
    "threshold = 300\n",
    "within_threshold = np.sum(absolute_errors <= threshold)\n",
    "total_predictions = len(y_test)\n",
    "percentage_within_threshold = (within_threshold / total_predictions) * 100\n",
    "\n",
    "print(f\"Percentage of predictions within ±{threshold} Elo: {percentage_within_threshold:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['(3,5]'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 18\u001b[0m\n\u001b[0;32m     12\u001b[0m mistake_labels \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(3,5]\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(5,10]\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(10,15]\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(15,20]\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(20,25]\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(25,30]\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(30,35]\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(35,40]\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(40,50]\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(50,60]\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m     15\u001b[0m ]\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Handle missing values in mistake intervals\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m summary_table_white[mistake_labels] \u001b[38;5;241m=\u001b[39m summary_table_white[mistake_labels]\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Ensure 'Elo' is numeric\u001b[39;00m\n\u001b[0;32m     21\u001b[0m summary_table_white[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mElo\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_numeric(summary_table_white[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mElo\u001b[39m\u001b[38;5;124m'\u001b[39m], errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoerce\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\foivo\\anaconda3\\envs\\guess_the_elo-env\\Lib\\site-packages\\pandas\\core\\frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39m_get_indexer_strict(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\foivo\\anaconda3\\envs\\guess_the_elo-env\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6200\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[0;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\foivo\\anaconda3\\envs\\guess_the_elo-env\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6252\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6251\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m-> 6252\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['(3,5]'] not in index\""
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load your summary_table\n",
    "#summary_table = pd.read_csv(\"../huge_analyzed_games/big_summary_table.csv\")\n",
    "\n",
    "# Filter data for White players only\n",
    "summary_table_white = summary_table[summary_table['Player'] == 'White']\n",
    "\n",
    "# Define the mistake intervals and labels\n",
    "mistake_labels = [\n",
    "    '(3,5]', '(5,10]', '(10,15]', '(15,20]', '(20,25]', '(25,30]',\n",
    "    '(30,35]', '(35,40]', '(40,50]', '(50,60]', \n",
    "]\n",
    "\n",
    "# Handle missing values in mistake intervals\n",
    "summary_table_white[mistake_labels] = summary_table_white[mistake_labels].fillna(0)\n",
    "\n",
    "# Ensure 'Elo' is numeric\n",
    "summary_table_white['Elo'] = pd.to_numeric(summary_table_white['Elo'], errors='coerce')\n",
    "\n",
    "# Remove rows with missing 'Elo' values\n",
    "summary_table_white = summary_table_white.dropna(subset=['Elo'])\n",
    "\n",
    "# Step 1: Bin the Elo Ratings\n",
    "# Define Elo rating bins\n",
    "elo_bins = [0, 1200, 1400, 1600, 1800, 2000, 2200, 2400, 2600, 2800, np.inf]\n",
    "elo_labels = ['<1200', '1200-1399', '1400-1599', '1600-1799', '1800-1999',\n",
    "              '2000-2199', '2200-2399', '2400-2599', '2600-2799', '2800+']\n",
    "\n",
    "# Assign Elo bins to the data\n",
    "summary_table_white['EloBin'] = pd.cut(\n",
    "    summary_table_white['Elo'],\n",
    "    bins=elo_bins,\n",
    "    labels=elo_labels,\n",
    "    right=False  # Left-inclusive intervals\n",
    ")\n",
    "\n",
    "# Step 2: Aggregate the Data\n",
    "# Group by EloBin and calculate the average number of mistakes in each category\n",
    "avg_mistakes_per_bin = summary_table_white.groupby('EloBin')[mistake_labels].mean().reset_index()\n",
    "\n",
    "# Step 3: Plot the Data\n",
    "# Set up the plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot a line for each mistake category\n",
    "for label in mistake_labels:\n",
    "    plt.plot(\n",
    "        avg_mistakes_per_bin['EloBin'],\n",
    "        avg_mistakes_per_bin[label],\n",
    "        marker='o',\n",
    "        label=label\n",
    "    )\n",
    "\n",
    "# Customize the plot\n",
    "plt.title('Average Number of Mistakes per Elo Rating Bin (White Players)')\n",
    "plt.xlabel('Elo Rating Bin')\n",
    "plt.ylabel('Average Number of Mistakes')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Mistake Category', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mistake_percentage(summary_table, interval_label):\n",
    "    \"\"\"\n",
    "    Calculates the percentage of games that have at least one mistake in the specified interval.\n",
    "\n",
    "    Parameters:\n",
    "    summary_table (pd.DataFrame): The DataFrame containing game summaries.\n",
    "    interval_label (str): The label of the mistake interval to analyze (e.g., '(25,30]').\n",
    "\n",
    "    Returns:\n",
    "    float: The percentage of games with at least one mistake in the specified interval.\n",
    "    \"\"\"\n",
    "    # Check if the interval column exists in the summary table\n",
    "    if interval_label not in summary_table.columns:\n",
    "        raise ValueError(f\"The interval '{interval_label}' does not exist in the summary table columns.\")\n",
    "\n",
    "    # Total number of games\n",
    "    total_games = len(summary_table)\n",
    "\n",
    "    # Number of games with at least one mistake in the specified interval\n",
    "    games_with_mistake = summary_table[summary_table[interval_label] > 0]\n",
    "    number_with_mistake = len(games_with_mistake)\n",
    "\n",
    "    # Compute the percentage\n",
    "    percentage = (number_with_mistake / total_games) * 100\n",
    "    print(f\"Percentage of games with at least one mistake in {interval_label} interval: {percentage:.2f}%\")\n",
    "    return percentage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of games with at least one mistake in (25,30] interval: 20.59%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20.58907891749974"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_mistake_percentage(summary_table, '(25,30]')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "guess_the_elo-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
